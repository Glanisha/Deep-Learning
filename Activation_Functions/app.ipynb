{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6aed38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: Maps to (0,1). Prone to vanishing gradients.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    \"\"\"Derivative of sigmoid.\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Tanh activation: Maps to (-1,1). Zero-centered.\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(x):\n",
    "    \"\"\"Derivative of tanh.\"\"\"\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation: Fast, but can cause dying neurons.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def d_relu(x):\n",
    "    \"\"\"Derivative of ReLU.\"\"\"\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU: Allows small gradient for x <= 0 to prevent dying neurons.\"\"\"\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def d_leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Derivative of Leaky ReLU.\"\"\"\n",
    "    return np.where(x > 0, 1, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37adea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc56020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2553\n",
      "Epoch 1000, Loss: 0.0077\n",
      "Epoch 2000, Loss: 0.0022\n",
      "Epoch 3000, Loss: 0.0012\n",
      "Epoch 4000, Loss: 0.0008\n",
      "Final Predictions (sigmoid output): [0.0411175  0.98157763 0.98629735 0.01380053]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.0411175 , 0.98157763, 0.98629735, 0.01380053])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Network architecture (unchanged)\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Define sigmoid function\n",
    "sigmoid = lambda z: 1 / (1 + np.exp(-z))\n",
    "\n",
    "def train_network(activation, d_activation, learning_rate, epochs=5000):\n",
    "    # Hardcoded weights with formulas (instead of random)\n",
    "    # W1: 2x4 matrix with patterned values for symmetry breaking\n",
    "    W1 = np.array([[0.5, -0.2, 0.1, 0.4],\n",
    "                   [-0.3, 0.8, -0.5, 0.2]])\n",
    "    # b1: Small incremental biases\n",
    "    b1 = np.array([[0.1, -0.1, 0.05, 0.0]])\n",
    "    # W2: 4x1 matrix with mixed signs\n",
    "    W2 = np.array([[0.4],\n",
    "                   [-0.6],\n",
    "                   [0.3],\n",
    "                   [0.2]])\n",
    "    # b2: Small constant bias\n",
    "    b2 = np.array([[0.05]])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        z1 = np.dot(X, W1) + b1  # Added z1 for derivative clarity\n",
    "        a1 = activation(z1)\n",
    "        z2 = np.dot(a1, W2) + b2  # Added z2 for derivative clarity\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        # Loss (MSE)\n",
    "        loss = np.mean((y - a2) ** 2)\n",
    "        \n",
    "        # Backpropagation\n",
    "        d_a2 = a2 - y\n",
    "        d_z2 = d_a2 * (a2 * (1 - a2))  # d_sigmoid using a2\n",
    "        d_W2 = np.dot(a1.T, d_z2)\n",
    "        d_b2 = np.sum(d_z2, axis=0, keepdims=True)\n",
    "        \n",
    "        d_a1 = np.dot(d_z2, W2.T)\n",
    "        d_z1 = d_a1 * d_activation(z1)  # Use stored z1\n",
    "        d_W1 = np.dot(X.T, d_z1)\n",
    "        d_b1 = np.sum(d_z1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient descent updates\n",
    "        W2 -= learning_rate * d_W2\n",
    "        b2 -= learning_rate * d_b2\n",
    "        W1 -= learning_rate * d_W1\n",
    "        b1 -= learning_rate * d_b1\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    # Final predictions are sigmoid output values, no thresholding\n",
    "    predictions = a2.flatten()\n",
    "    print(\"Final Predictions (sigmoid output):\", predictions)\n",
    "    print(\"-\" * 50)\n",
    "    return predictions\n",
    "\n",
    "# Sample data for test (XOR example)\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Define example activations (ReLU and its derivative) for testing\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "def d_relu(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "# Run modified training with hardcoded weights\n",
    "train_network(relu, d_relu, learning_rate=0.1, epochs=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09240779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Using Sigmoid Activation, LR=0.1 ===\n",
      "Epoch 0, Loss: 0.2530\n",
      "Epoch 1000, Loss: 0.2500\n",
      "Epoch 2000, Loss: 0.2496\n",
      "Epoch 3000, Loss: 0.2478\n",
      "Epoch 4000, Loss: 0.2281\n",
      "Final Predictions (sigmoid output): [0.22660326 0.50993486 0.7518223  0.4932818 ]\n",
      "--------------------------------------------------\n",
      "=== Using Tanh Activation, LR=0.1 ===\n",
      "Epoch 0, Loss: 0.2577\n",
      "Epoch 1000, Loss: 0.0151\n",
      "Epoch 2000, Loss: 0.0033\n",
      "Epoch 3000, Loss: 0.0017\n",
      "Epoch 4000, Loss: 0.0012\n",
      "Final Predictions (sigmoid output): [0.01336453 0.96785792 0.96869476 0.0363606 ]\n",
      "--------------------------------------------------\n",
      "=== Using ReLU Activation, LR=0.01 ===\n",
      "Epoch 0, Loss: 0.2553\n",
      "Epoch 1000, Loss: 0.2048\n",
      "Epoch 2000, Loss: 0.1587\n",
      "Epoch 3000, Loss: 0.1284\n",
      "Epoch 4000, Loss: 0.0848\n",
      "Final Predictions (sigmoid output): [0.31953311 0.76920593 0.87433917 0.15196081]\n",
      "--------------------------------------------------\n",
      "=== Using Leaky ReLU Activation, LR=0.01 ===\n",
      "Epoch 0, Loss: 0.2553\n",
      "Epoch 1000, Loss: 0.2061\n",
      "Epoch 2000, Loss: 0.1610\n",
      "Epoch 3000, Loss: 0.1313\n",
      "Epoch 4000, Loss: 0.0876\n",
      "Final Predictions (sigmoid output): [0.32464031 0.76538026 0.87026712 0.15579364]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.32464031, 0.76538026, 0.87026712, 0.15579364])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try different activation functions and learning rates\n",
    "\n",
    "print(\"=== Using Sigmoid Activation, LR=0.1 ===\")\n",
    "train_network(sigmoid, d_sigmoid, learning_rate=0.1)\n",
    "\n",
    "print(\"=== Using Tanh Activation, LR=0.1 ===\")\n",
    "train_network(tanh, d_tanh, learning_rate=0.1)\n",
    "\n",
    "print(\"=== Using ReLU Activation, LR=0.01 ===\")\n",
    "train_network(relu, d_relu, learning_rate=0.01)\n",
    "\n",
    "print(\"=== Using Leaky ReLU Activation, LR=0.01 ===\")\n",
    "train_network(leaky_relu, d_leaky_relu, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae2be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training with SIGMOID | LR: 0.5 ---\n",
      "Epoch 0, Loss: 0.250132\n",
      "Epoch 2000, Loss: 0.250000\n",
      "Epoch 4000, Loss: 0.250000\n",
      "Epoch 6000, Loss: 0.250000\n",
      "Epoch 8000, Loss: 0.250000\n",
      "Epoch 10000, Loss: 0.250000\n",
      "Input: [0,0], [0,1], [1,0], [1,1]\n",
      "Predicted Output (binary): [[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "Expected Output:         [0 1 1 0]\n",
      "Final Accuracy: 50.00%\n",
      "Training Time: 1.81 seconds\n",
      "\n",
      "--- Training with TANH | LR: 0.5 ---\n",
      "Epoch 0, Loss: 0.250028\n",
      "Epoch 2000, Loss: 0.001220\n",
      "Epoch 4000, Loss: 0.000237\n",
      "Epoch 6000, Loss: 0.000126\n",
      "Epoch 8000, Loss: 0.000085\n",
      "Epoch 10000, Loss: 0.000064\n",
      "Input: [0,0], [0,1], [1,0], [1,1]\n",
      "Predicted Output (binary): [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Expected Output:         [0 1 1 0]\n",
      "Final Accuracy: 50.00%\n",
      "Training Time: 1.07 seconds\n",
      "\n",
      "--- Training with RELU | LR: 0.1 ---\n",
      "Epoch 0, Loss: 0.250081\n",
      "Epoch 2000, Loss: 0.166924\n",
      "Epoch 4000, Loss: 0.166747\n",
      "Epoch 6000, Loss: 0.166718\n",
      "Epoch 8000, Loss: 0.166704\n",
      "Epoch 10000, Loss: 0.166693\n",
      "Input: [0,0], [0,1], [1,0], [1,1]\n",
      "Predicted Output (binary): [[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Expected Output:         [0 1 1 0]\n",
      "Final Accuracy: 50.00%\n",
      "Training Time: 1.10 seconds\n",
      "\n",
      "--- Training with LEAKY_RELU | LR: 0.1 ---\n",
      "Epoch 0, Loss: 0.250080\n",
      "Epoch 2000, Loss: 0.169999\n",
      "Epoch 4000, Loss: 0.169968\n",
      "Epoch 6000, Loss: 0.129935\n",
      "Epoch 8000, Loss: 0.129792\n",
      "Epoch 10000, Loss: 0.129791\n",
      "Input: [0,0], [0,1], [1,0], [1,1]\n",
      "Predicted Output (binary): [[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Expected Output:         [0 1 1 0]\n",
      "Final Accuracy: 50.00%\n",
      "Training Time: 1.18 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def d_relu(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def d_leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    'sigmoid': (sigmoid, d_sigmoid),\n",
    "    'tanh': (tanh, d_tanh),\n",
    "    'relu': (relu, d_relu),\n",
    "    'leaky_relu': (leaky_relu, d_leaky_relu)\n",
    "}\n",
    "\n",
    "\n",
    "def train_network(X, y, activation_name, learning_rate, epochs=10000):\n",
    "    print(f\"--- Training with {activation_name.upper()} | LR: {learning_rate} ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    input_size, hidden_size, output_size = 2, 4, 1\n",
    "    activation_func, d_activation_func = ACTIVATIONS[activation_name]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
    "    b2 = np.zeros((1, output_size))\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "     \n",
    "        z1 = np.dot(X, W1) + b1\n",
    "        a1 = activation_func(z1)\n",
    "        z2 = np.dot(a1, W2) + b2\n",
    "        a2 = sigmoid(z2) \n",
    "\n",
    "        loss = np.mean((y - a2)**2)\n",
    "\n",
    "    \n",
    "        d_loss_a2 = a2 - y\n",
    "        d_loss_z2 = d_loss_a2 * (a2 * (1 - a2))\n",
    "        d_loss_W2 = np.dot(a1.T, d_loss_z2)\n",
    "        d_loss_b2 = np.sum(d_loss_z2, axis=0, keepdims=True)\n",
    "        d_loss_a1 = np.dot(d_loss_z2, W2.T)\n",
    "        d_loss_z1 = d_loss_a1 * d_activation_func(z1)\n",
    "        d_loss_W1 = np.dot(X.T, d_loss_z1)\n",
    "        d_loss_b1 = np.sum(d_loss_z1, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "        W1 -= learning_rate * d_loss_W1\n",
    "        W2 -= learning_rate * d_loss_W2\n",
    "        b1 -= learning_rate * d_loss_b1\n",
    "        b2 -= learning_rate * d_loss_b2\n",
    "        \n",
    "        if epoch % 2000 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "    \n",
    "    final_predictions_binary = (a2 > 0.5).astype(int)\n",
    "    accuracy = np.mean(final_predictions_binary == y.flatten()) * 100\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Epoch {epochs}, Loss: {loss:.6f}\")\n",
    "    print(f\"Input: [0,0], [0,1], [1,0], [1,1]\")\n",
    "    print(f\"Predicted Output (binary): {final_predictions_binary}\")\n",
    "    print(f\"Expected Output:         {y.flatten()}\")\n",
    "    print(f\"Final Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Training Time: {end_time - start_time:.2f} seconds\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "    experiments = [\n",
    "        {'activation': 'sigmoid', 'lr': 0.5},\n",
    "        {'activation': 'tanh', 'lr': 0.5},\n",
    "        {'activation': 'relu', 'lr': 0.1},\n",
    "        {'activation': 'leaky_relu', 'lr': 0.1}\n",
    "    ]\n",
    "\n",
    "    for exp in experiments:\n",
    "        train_network(X, y, exp['activation'], exp['lr'], epochs=10000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
